Во первых, нужно сжато описать текущую нагрузку на систему(только после этого можно обсуждать её рост, что произойдет, если количество запросов удвоится?). Нагрузку можно описать с помощью нескольких параметров. У приложений они разные, где-то количество пользователей, которые смотрят один стрим, или несколько и много, где-то количество операций на запись в базу и чтение, важно понять, какие параметры важны для Вашего приложения.

Пример с Твитером 2012 года. Две основные метрики
1. Публикации твита - юзер публикует новое сообщение для своих подписчиков
2. Домашняя лента - юзер просматривает сообщения, на кого он подписан

Обработка 12 тыс. записей в секунду было не сложно и система справлялась. Проблема с мастабированием заключалась в том, что [[Коэфициент разветвления]] очень высок, пользователь может быть подписан на гигантское количество других пользователей и наоборот. в голову приходит два варианта реализации

1. Простой запрос в базу, с джойнами и сортировкой по дате создания пользователя
2. Заранее высчитывать домашнюю ленту для человека и записывать в кеш.

Собственно твитер использовал первую модель вначале, но столкнулся с проблемой быстрой доставкой домашних лент. Люди создавали новый твит в 2-3 раза реже, чем запрашивали домашнюю ленту, поэтому компания постепенно перешла ко второму решению. 

Позже, компанию узнала недостатки второго варианта, помимо огромного объема кеша, в среднем в то время твит доставлялся 75 подписчикам, если публикуют примерно 4,6 тыс. твитов в секунду, то это нужно пересчитать домашнюю ленту для 345 тысячам человек. Это средняя статистика, но есть супер крупные и медийные личности, у которых по 30 млн подписчиков, стало сложно выдавать твиты, хотя твитер старается выдавать твиты в течении 5 секунд.

На примере твитера можно увидеть, что их ключевой параметр для масташбирования, это доставка твита людям, именно оно определяется нагрузку разветвления по выходу. У всех разные приложения, но рассуждения относительно их нагрузки можно применять аналогичные принципы.

И последний поворот в этой истории. Когда твитер реализовала ошибкоустойчивый подход, компания постепенно двигалась к чему-то среднему 1ого и 2ого варианта. Компания использовала кеш специально и отдельно для супер популярных пользователей, от 1 млн, а для остальных пользователей использовала первый подход и после объединяли ленты.

